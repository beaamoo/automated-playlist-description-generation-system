{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playlist Title Generation using RNN and Transformer Models\n",
    "\n",
    "In this notebook, we explore the task of generating titles for music playlists by implementing and comparing two neural network architectures: Recurrent Neural Networks (RNN) and Transformer models. Our goal is to evaluate how well each model performs on this task and to understand their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import all the necessary libraries we'll need to load the data, define our models, and train and evaluate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bestricemossberg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure you've downloaded the NLTK tokenizer's dataset\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "We have preprocessed and split our dataset into training, validation, and test sets. Now, we'll load these datasets from their respective JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_data = load_data('../datasets/train.json')\n",
    "val_data = load_data('../datasets/validation.json')\n",
    "test_data = load_data('../datasets/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to intialize the decoder model\n",
    "max_seq_length = max(len(item['description']) for item in train_data) + 1  # Adding 1 for the EOS token\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Training\n",
    "Convert the playlists and descriptions into sequences of tokens, pad them to uniform length, and create DataLoader instances for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sequences\n",
    "UNK_token = 1  # Represents unknown words not in the vocabulary\n",
    "SOS_token = 2  # Start-of-sequence token\n",
    "EOS_token = 3  # End-of-sequence token\n",
    "\n",
    "def build_vocab(sequences):\n",
    "    \"\"\"Build a vocabulary from a list of sequences.\"\"\"\n",
    "    counter = Counter()\n",
    "    for seq in sequences:\n",
    "        counter.update(seq)\n",
    "    # Start indexing from 4 to leave space for special tokens\n",
    "    vocab = {word: i+4 for i, (word, _) in enumerate(counter.items())}\n",
    "    vocab['<PAD>'] = PAD_token\n",
    "    vocab['<UNK>'] = UNK_token\n",
    "    vocab['<SOS>'] = SOS_token\n",
    "    vocab['<EOS>'] = EOS_token\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Flatten lists of tokens for tracks and descriptions\n",
    "all_tracks = [track for item in train_data for track in item['tracks']]\n",
    "all_descriptions = [token for item in train_data for token in word_tokenize(' '.join(item['description']).lower())]\n",
    "\n",
    "# Build vocabularies\n",
    "track_vocab = build_vocab(all_tracks)\n",
    "description_vocab = build_vocab(all_descriptions)\n",
    "\n",
    "# Check the size of each vocabulary\n",
    "track_vocab_size = len(track_vocab)\n",
    "description_vocab_size = len(description_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function encodes a list of textual tokens into their corresponding numerical IDs based on a given vocabulary. \n",
    "# If a token is not found in the vocabulary, it uses a special <UNK> token to represent unknown words.\n",
    "def encode_sequence(sequence, vocabulary, add_eos=False):\n",
    "    \"\"\"Encode a sequence of tokens into a sequence of numerical IDs.\"\"\"\n",
    "    encoded_sequence = [vocabulary.get(token, UNK_token) for token in sequence]\n",
    "    if add_eos:\n",
    "        encoded_sequence.append(EOS_token)\n",
    "    return encoded_sequence\n",
    "\n",
    "# This function takes a list of sequences and pads them to the same length by adding a specified pad_token \n",
    "# (default is 0) to the end of shorter sequences. This is necessary for batch processing in models that require \n",
    "# input sequences of uniform length.\n",
    "def pad_sequences(sequences, pad_token=0):\n",
    "    \"\"\"Pad sequences to the same length with a pad_token.\"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences) # Determine the maximum length of any sequence in the batch\n",
    "    padded_sequences = [seq + [pad_token] * (max_length - len(seq)) for seq in sequences] # Pad each sequence to match the max length by appending the pad_token\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "# Custom Dataset class for handling the playlist dataset. It requires the dataset itself and vocabularies for \n",
    "# both tracks and descriptions. The __getitem__ method returns the encoded and tensorized versions of the tracks and descriptions.\n",
    "class PlaylistDataset(Dataset):\n",
    "    def __init__(self, data, track_vocab, description_vocab):\n",
    "        self.data = data\n",
    "        self.track_vocab = track_vocab\n",
    "        self.description_vocab = description_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        track_ids = encode_sequence(item['tracks'], self.track_vocab)\n",
    "        description_ids = encode_sequence(item['description'], self.description_vocab)\n",
    "        return torch.tensor(track_ids), torch.tensor(description_ids)\n",
    "\n",
    "# Custom collate function for DataLoader. It pads the sequences in each batch to ensure they have the same length, \n",
    "# allowing them to be processed together as a batch. This function is passed to the DataLoader to be applied to each batch.\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader.\"\"\"\n",
    "    # Extract tracks and descriptions from the batch\n",
    "    tracks, descriptions = zip(*[(item[0], item[1]) for item in batch])\n",
    "    # Encode and pad sequences\n",
    "    tracks_encoded = [encode_sequence(track, track_vocab) for track in tracks]\n",
    "    descriptions_encoded = [encode_sequence(description, description_vocab, add_eos=True) for description in descriptions]\n",
    "\n",
    "    tracks_padded = pad_sequences(tracks_encoded, pad_token=track_vocab['<PAD>'])\n",
    "    descriptions_padded = pad_sequences(descriptions_encoded, pad_token=description_vocab['<PAD>'])\n",
    "    \n",
    "    # Convert to tensors and return\n",
    "    return torch.tensor(tracks_padded, dtype=torch.long), torch.tensor(descriptions_padded, dtype=torch.long)\n",
    "\n",
    "# Setting the batch size for the DataLoader. The DataLoader iterates over the PlaylistDataset in batches, using the collate_fn to pad the sequences in each batch.\n",
    "batch_size = 32\n",
    "\n",
    "# Instantiating the DataLoader with the PlaylistDataset. The DataLoader facilitates efficient iteration over the dataset during the training process.\n",
    "train_loader = DataLoader(PlaylistDataset(train_data, track_vocab, description_vocab),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder will use a bidirectional GRU to process the input sequence of track IDs, producing a set of hidden states that represent the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers=2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        # Ensure bidirectional=True is compatible with your overall model design\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)  # input_seq: [seq_len, batch_size]\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        # Sum bidirectional GRU outputs if using bidirectional. Adjust as needed.\n",
    "        outputs = (outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:])\n",
    "        return outputs, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        # Determine the multiplier for bidirectional GRUs\n",
    "        bidirectional_multiplier = 2 if self.gru.bidirectional else 1\n",
    "        return torch.zeros(self.num_layers * bidirectional_multiplier, batch_size, self.hidden_size, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with Attention Mechanism\n",
    "The decoder uses a unidirectional GRU along with an attention mechanism. The attention mechanism allows the decoder to focus on different parts of the input sequence for each step of the output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, max_length, num_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2 + embed_size, max_length)  # Adjust for correct size\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2 + embed_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(embed_size + self.hidden_size * 2, hidden_size, num_layers=num_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).unsqueeze(1)  # [batch_size, 1, embed_size]\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # For attention calculation, ensure dimensions are compatible\n",
    "        # Adjust hidden to have same dim as embedded for concatenation\n",
    "        hidden_for_concat = hidden.transpose(0, 1).contiguous().view(1, hidden.size(1), -1)  # Reshape hidden for batch processing\n",
    "        \n",
    "        # Concatenate along the dimension that matches embedded size\n",
    "        attn_input = torch.cat((embedded, hidden_for_concat), dim=2)  # [batch_size, 1, embed_size + hidden_size * num_layers * num_directions]\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(attn_input), dim=2)  # Adjust softmax dim if necessary\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1))  # Apply attention to encoder outputs\n",
    "        \n",
    "        # Combine attended encoder outputs with embedded input before sending to GRU\n",
    "        output = torch.cat((embedded, attn_applied), dim=2)\n",
    "        output = self.attn_combine(output)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output.squeeze(1)), dim=1)  # Adjust for batch processing\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        bidirectional_multiplier = 2 if self.gru.bidirectional else 1  # Use 2 if your GRU is bidirectional; adjust as necessary\n",
    "        return torch.zeros(self.num_layers * bidirectional_multiplier, batch_size, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Training Process\n",
    "\n",
    "Before writing the training loop, define the loss function and optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming SOS_token, EOS_token, epochs, device, and log_interval are defined\n",
    "SOS_token = 2  # Arbitrary integer to represent the start of a sequence\n",
    "EOS_token = 3  # Arbitrary integer to represent the end of a sequence\n",
    "epochs = 10  # Number of training epochs, adjust based on your dataset and model performance\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_interval = 5  # Log training info every 5 epochs, adjust as needed\n",
    "\n",
    "# Adjust embed_size and hidden_size as per specifications\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "\n",
    "encoder = EncoderRNN(input_size=len(track_vocab), embed_size=embed_size, hidden_size=hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(embed_size=embed_size, hidden_size=hidden_size, output_size=len(description_vocab), max_length=max_seq_length, num_layers=2).to(device)  # Ensure AttnDecoderRNN is defined similarly\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function and Training Loop\n",
    "The training loop involves processing each batch of data through the encoder and decoder, calculating the loss, and updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, batch_size):\n",
    "    # Initialize the encoder's hidden state for the current batch size\n",
    "    encoder_hidden = encoder.initHidden(batch_size).to(device)\n",
    "\n",
    "    # Reset gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Define the initial input and hidden state for the decoder\n",
    "    decoder_input = torch.tensor([[SOS_token]] * batch_size, device=device)  # Adjust for batch_size\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Determine the lengths of the input and target sequences\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    loss = 0\n",
    "\n",
    "    # Forward pass through the encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "            decoder_input = target_tensor[di]  # Next input comes from the teaching data\n",
    "    else:\n",
    "        # Without teacher forcing: use the decoder's own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze(1).detach()  # Detach from history as input\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m input_tensor, target_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mto(device), target_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming input_tensor is [seq_len, batch_size]\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass batch_size to train\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[75], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_teacher_forcing:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Teacher forcing: Feed the target as the next input\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m di \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(target_length):\n\u001b[0;32m---> 29\u001b[0m         decoder_output, decoder_hidden, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(decoder_output, target_tensor[di]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     31\u001b[0m         decoder_input \u001b[38;5;241m=\u001b[39m target_tensor[di]  \u001b[38;5;66;03m# Next input comes from the teaching data\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 30\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m hidden_for_concat \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, hidden\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape hidden for batch processing\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Concatenate along the dimension that matches embedded size\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m attn_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_for_concat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, 1, embed_size + hidden_size * num_layers * num_directions]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(attn_input), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Adjust softmax dim if necessary\u001b[39;00m\n\u001b[1;32m     33\u001b[0m attn_applied \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn_weights, encoder_outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Apply attention to encoder outputs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 3"
     ]
    }
   ],
   "source": [
    "# Assuming train_loader is defined and ready\n",
    "epochs = 10\n",
    "teacher_forcing_ratio = 0.5\n",
    "print_every = 1000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    for i, (input_tensor, target_tensor) in enumerate(train_loader, 1):\n",
    "        input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
    "        batch_size = input_tensor.size(1)  # Assuming input_tensor is [seq_len, batch_size]\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_seq_length, batch_size=batch_size)  # Pass batch_size to train\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(f\"Step {i}, Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Playlist Descriptions\n",
    "After training, you can use the model to generate playlist descriptions by processing a sequence of track IDs through the encoder and iteratively predicting the next word in the description with the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(encoder, decoder, track_sequence, max_length):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = track_sequence.to(device)\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden().to(device)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(topi.item())\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_description(encoder, decoder, track_sequence, max_length, vocab):\n",
    "    # Assuming 'vocab' is a dictionary mapping IDs back to words\n",
    "    with torch.no_grad():\n",
    "        # similar setup as before\n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            # similar decoding steps as before\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(vocab[topi.item()])  # Convert ID back to word\n",
    "\n",
    "        return decoded_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
